{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2d7eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "class Basket:\n",
    "    def __init__(self, fruits: list):\n",
    "        self.fruits = fruits\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.fruits)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> str:\n",
    "        return self.fruits[index]\n",
    "    \n",
    "basket = Basket(['apple', 'banana', 'mango'])\n",
    "\n",
    "print(len(basket))\n",
    "print(basket[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b328ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    BertForTokenClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup ###### Gradually warms up then decays learning rate for stable BERT training.\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW #Adam Optimizer with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d874350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        text = str(self.texts[index])\n",
    "        label = int(self.labels[index])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation = True,\n",
    "            padding = 'max_length',\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "        \"\"\"\n",
    "        {\n",
    "            'input_ids': tensor([101, 2023, 2003, 1037, ...]),\n",
    "            'attention_mask': tensor([1, 1, 1, 0, 0, ...]),\n",
    "            'labels': tensor(1)\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT text classifier\n",
    "\n",
    "class BERTTextClassifier:\n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_classes = num_classes\n",
    "        )\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    def load_imdb_dataset(self, sample_size=5000):\n",
    "        print(\"Loading IMDB dataset ...\")\n",
    "\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "\n",
    "        # sample data from faster training \n",
    "        train_indices = np.random.choice(\n",
    "            len(dataset['train']), # n, pick random int from 0 - (n-1)\n",
    "            min(sample_size, len(dataset['train'])), # num of sample need (this logic - pick only what in dataset )\n",
    "            replace=False # pick unique samples only\n",
    "        )\n",
    "\n",
    "        test_indices = np.random.choice(\n",
    "            len(dataset['test']),\n",
    "            min(sample_size//4, len(dataset['test'])),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # convert numpy.int64 -> int for indexing\n",
    "        train_texts = [dataset['train'][int(i)]['text'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['label'] for i in train_indices]\n",
    "\n",
    "        test_texts = [dataset['test'][int(i)]['text'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['label'] for i in test_indices]\n",
    "\n",
    "        print(f\"train samples: {len(train_texts)}\")\n",
    "        print(f\"test samples: {len(test_texts)}\")\n",
    "\n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "\n",
    "    def train(self, train_texts, train_labels, epochs=1, batch_size=8, learning_rate=2e-5) -> None:\n",
    "        train_dataset = TextClassificationDataset(\n",
    "            train_texts, train_labels, self.tokenizer, self.max_length\n",
    "        ) # get dataset, each get tokenized(truned into token id, attention maske)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # make batches\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01) # update model weights based on gradients\n",
    "\n",
    "        total_steps = len(train_loader) * epochs # total number of training steps (batches) across all epochs\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        ) # Adjusts learning rate as training progresses\n",
    "\n",
    "        self.model.train() \n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar: # batch in train_loader\n",
    "                optimizer.zero_grad() # clears out previous gradients\n",
    "                \n",
    "                # load everything to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids = input_ids,\n",
    "                    attention_mask = attention_mask,\n",
    "                    labels = labels\n",
    "                )\n",
    "\n",
    "                # get the loss\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item() # add to total loss\n",
    "\n",
    "                # backward pass - \n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) # This prevents exploding gradients, especially in large models. If any gradient exceeds 1.0, itâ€™s scaled down proportionally.\n",
    "\n",
    "                optimizer.step() # update model weight - weight = weight - learning_rate * gradient\n",
    "\n",
    "                scheduler.step() # \n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            avg_loss = total_loss/len(train_loader)\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "    def evaluate(self, test_texts, test_labels, batch_size=8):\n",
    "        \n",
    "        test_dataset = TextClassificationDataset(\n",
    "            test_texts, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad(): # prevents PyTorch from building a computational graph (no need backpropagration)\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # - The model predicts logits, which are raw, unnormalized scores (not yet probabilities).\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy() # # preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(true_labels, predictions)\n",
    "            f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "            report = classification_report(true_labels, predictions, target_names=['Nagative', 'Positive'])\n",
    "\n",
    "            return accuracy, f1, report\n",
    "\n",
    "\n",
    "    def predict(self, texts):\n",
    "        predictions = [] # what model predict\n",
    "        probabilities = [] # true values\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for text in texts:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad(): \n",
    "                # - The model predicts logits, which are raw, unnormalized scores (not yet probabilities).\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                pred = torch.argmax(logits, dim=1).cpu().numpy()[0] \n",
    "\n",
    "\n",
    "                predictions.append(pred)\n",
    "                probabilities.append(probs)\n",
    "\n",
    "        return predictions, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abb3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if batch size is 5 that mean something like this\n",
    "\"\"\"\n",
    "{\n",
    "  'input_ids': tensor([\n",
    "      [101, 2023, 2003, ..., 0, 0],\n",
    "      [101, 2054, 2003, ..., 0, 0],\n",
    "      [101, 1045, 2293, ..., 0, 0],\n",
    "      [101, 2009, 2003, ..., 0, 0],\n",
    "      [101, 2057, 2024, ..., 0, 0]\n",
    "  ]),\n",
    "  'attention_mask': tensor([\n",
    "      [1, 1, 1, 1, 0, 0, ...],\n",
    "      [1, 1, 1, 0, 0, 0, ...],\n",
    "      [1, 1, 1, 1, 1, 0, ...],\n",
    "      [1, 1, 1, 0, 0, 0, ...],\n",
    "      [1, 1, 1, 1, 1, 0, ...]\n",
    "  ]),\n",
    "  'labels': tensor([1, 0, 1, 0, 1])\n",
    "}\n",
    "\n",
    "not ->\n",
    "\n",
    "[ \n",
    "    { 'input_ids', 'attention_mask', 'labels' }, \n",
    "    { 'input_ids', 'attention_mask', 'labels' }, \n",
    "    { 'input_ids', 'attention_mask', 'labels' }, \n",
    "    { 'input_ids', 'attention_mask', 'labels' }, \n",
    "    { 'input_ids', 'attention_mask', 'labels' }, \n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3c072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616caa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
